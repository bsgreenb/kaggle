{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1a1bd1bbe61448acab6bb1059d3e21f7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.sv…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from huggingface_hub import notebook_login\n",
    "\n",
    "notebook_login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://www.kaggle.com/competitions/nlp-getting-started\n",
    "\n",
    "import numpy as np \n",
    "from datasets import load_dataset, DatasetDict\n",
    "from transformers import AutoTokenizer, DataCollatorWithPadding, AutoModelForSequenceClassification, TrainingArguments, Trainer\n",
    "import evaluate\n",
    "\n",
    "train_dataset = load_dataset('csv', data_files='train.csv')\n",
    "test_dataset = load_dataset('csv', data_files='test.csv')\n",
    "\n",
    "dataset = DatasetDict({\n",
    "    'train': train_dataset['train'],\n",
    "    'test': test_dataset['train']\n",
    "})\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"distilbert-base-uncased\")\n",
    "\n",
    "def preprocess_function(examples):\n",
    "    return tokenizer(examples[\"text\"], truncation=True)\n",
    "\n",
    "dataset['train'] = dataset['train'].rename_column(\"target\", \"labels\")\n",
    "dataset = dataset.map(preprocess_function, batched=True)\n",
    "\n",
    "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n",
    "\n",
    "metrics = evaluate.combine([\"accuracy\", \"f1\", \"precision\", \"recall\"])\n",
    "# metrics.compute(predictions=[0, 1, 0], references=[0, 1, 1])\n",
    "\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\"distilbert-base-uncased\", num_labels=2)\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"my_awesome_model\",\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=16,\n",
    "    num_train_epochs=2,\n",
    "    weight_decay=0.01,\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    load_best_model_at_end=True,\n",
    "    push_to_hub=True,\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=dataset['train'],\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=metrics,\n",
    ")\n",
    "\n",
    "trainer.train()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#TODO: may need to rename the label to label https://huggingface.co/docs/datasets/process#rename\n",
    "#TODO: make sure label is cast correctly, and do so if not already https://huggingface.co/docs/datasets/process#cast\n",
    "\n",
    "#TODO: make sure to shuffle the dataset when training (and take account of this in terms of IDS and predictions).  shuffling makes sense because the dataset is sorted, and BERT would be finetuned by SGD and order would have an effect.\n",
    "    # https://huggingface.co/docs/datasets/process#shuffle\n",
    "    # \" as soon as your Dataset has an indices mapping, the speed can become 10x slower. This is because there is an extra step to get the row index to read using the indices mapping, and most importantly, you aren’t reading contiguous chunks of data anymore. To restore the speed, you’d need to rewrite the entire dataset on your disk again using Dataset.flatten_indices(), which removes the indices mapping.\"\n",
    "    # Note: no need to shuffle during testing\n",
    "    # QSTN: reshuffle per epoch?\n",
    "\n",
    "\n",
    "#TODO: Load the automodel or whatev through pipeline.  Identify which model it uses by default for text-classification and read the docs on it.  I'm guessing it's BERT.\n",
    "\n",
    "#TODO: Preprocess (tokenize) the data.  this might be handled by pipeline dknow.\n",
    "    # https://huggingface.co/docs/datasets/nlp_process\n",
    "    # def tokenization(example):\n",
    "    #     return tokenizer(example[\"text\"])\n",
    "\n",
    "    # dataset = dataset.map(tokenization, batched=True)\n",
    "\n",
    "\n",
    "#TODO: see if including the fields beyond the text of the tweet impacts F1 score or not.  Let's do first pass without that stuff and see how leaderboard goes.\n",
    "\n",
    "\n",
    "\n",
    "#TODO: score on F1\n",
    "    # See https://huggingface.co/docs/evaluate/index\n",
    "\n",
    "\n",
    "#TODO: write results to CSV\n",
    "# For each ID in the test set, you must predict 1 if the tweet is describing a real disaster, and 0 otherwise. The file should contain a header and have the following format:\n",
    "# id,target\n",
    "# 0,0\n",
    "# 2,0\n",
    "# 3,1\n",
    "# 9,0\n",
    "# 11,0\n",
    "#sample_submission.to_csv(\"submission.csv\", index=False)\n",
    "\n",
    "#TODO: consider if stratification during training would help with F1.\n",
    "#TODO: look at other ppl's notebooks that used HF Transformers and see how I can improve score.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
