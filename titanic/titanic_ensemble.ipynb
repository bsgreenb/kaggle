{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters for XGBClassifier: {} score=0.822666499278137\n",
      "Best parameters for LogisticRegression: {} score=0.7878978093026175\n",
      "Best parameters for SGDClassifier: {} score=0.803603038101814\n",
      "Best parameters for KNeighborsClassifier: {} score=0.8181595631159375\n",
      "Best parameters for RandomForestClassifier: {} score=0.8294269035214361\n",
      "Best parameters for GaussianNB: {} score=0.7879103634423452\n",
      "Best parameters for AdaBoostClassifier: {} score=0.8137342288619672\n",
      "Best parameters for NuSVC: {} score=0.8282719226664993\n",
      "Best parameters for ExtraTreesClassifier: {} score=0.8249262444291006\n",
      "Best parameters for LinearDiscriminantAnalysis: {} score=0.7912497646098802\n",
      "Best parameters for MLPClassifier: {} score=0.8024919967359236\n",
      "Accuracy of the hard voting classifier: 0.8428731762065096\n",
      "Accuracy of the soft voting classifier: 0.8428731762065096\n"
     ]
    }
   ],
   "source": [
    "# Following section in book: Using the majority voting principle to make predictions\n",
    "\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV\n",
    "from sklearn.linear_model import LogisticRegression, SGDClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "from sklearn.ensemble import VotingClassifier, RandomForestClassifier, AdaBoostClassifier, ExtraTreesClassifier\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.svm import LinearSVC, NuSVC\n",
    "import xgboost as xgb\n",
    "import numpy as np\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "from sklearn.exceptions import ConvergenceWarning\n",
    "ConvergenceWarning('ignore')\n",
    "\n",
    "def feature_eng(data):\n",
    "    data['Embarked'].fillna(data['Embarked'].mode()[0], inplace = True)\n",
    "    data['FamilySize'] = data['SibSp'] + data['Parch'] + 1\n",
    "    data['Fare'].fillna(data['Fare'].mean(),inplace=True)\n",
    "    eng_title(data)\n",
    "    eng_age(data)\n",
    "\n",
    "def eng_title(data):\n",
    "    data['Title']=0\n",
    "    data['Title']=data.Name.str.extract('([A-Za-z]+)\\.') #lets extract the Salutations\n",
    "    data['Title'].replace(['Mlle','Mme','Ms','Dr','Major','Lady','Countess','Dona','Jonkheer','Col',\n",
    "                         'Rev','Capt','Sir','Don'],['Miss','Miss','Miss','Mr','Mr','Mrs','Mrs','Mrs','Other','Other','Other','Mr','Mr','Mr'],inplace=True)\n",
    "\n",
    "def eng_age(data):\n",
    "    data.loc[(data.Age.isnull())&(data.Title=='Mr'),'Age']= data.Age[data.Title==\"Mr\"].mean()\n",
    "    data.loc[(data.Age.isnull())&(data.Title=='Mrs'),'Age']= data.Age[data.Title==\"Mrs\"].mean()\n",
    "    data.loc[(data.Age.isnull())&(data.Title=='Master'),'Age']= data.Age[data.Title==\"Master\"].mean()\n",
    "    data.loc[(data.Age.isnull())&(data.Title=='Miss'),'Age']= data.Age[data.Title==\"Miss\"].mean()\n",
    "    data.loc[(data.Age.isnull())&(data.Title=='Other'),'Age']= data.Age[data.Title==\"Other\"].mean()\n",
    "\n",
    "\n",
    "train_data = pd.read_csv('train.csv')\n",
    "feature_eng(train_data)\n",
    "\n",
    "test_data = pd.read_csv('test.csv')\n",
    "feature_eng(test_data)\n",
    "\n",
    "features = ['Pclass', 'Sex', 'Age', 'SibSp', 'Parch', 'FamilySize', 'Fare', 'Embarked']\n",
    "\n",
    "X_train = train_data[features]\n",
    "y_train = train_data['Survived']\n",
    "\n",
    "X_test = test_data[features]\n",
    "\n",
    "X_train = pd.get_dummies(X_train)\n",
    "X_test = pd.get_dummies(X_test)\n",
    "\n",
    "# X_train, X_valid, y_train, y_valid = train_test_split(X_train_orig, y_train_orig, stratify=y_train_orig, random_state=1)\n",
    "\n",
    "all_clf = []\n",
    "scalers = []\n",
    "hyper_param_grid = {}\n",
    "\n",
    "def add_clf(clf, hyper_params = {}, scaler=StandardScaler):\n",
    "    all_clf.append(clf)\n",
    "    name = clf.__class__.__name__\n",
    "    hyper_param_grid[name] = hyper_params\n",
    "    scalers.append(scaler)\n",
    "\n",
    "add_clf(xgb.XGBClassifier(max_depth=4, learning_rate=.01, n_estimators=300,random_state=1), {}, None)\n",
    "add_clf(LogisticRegression(random_state=1, solver='lbfgs', max_iter=100, penalty='l2', C=.1)) \n",
    "add_clf(SGDClassifier(random_state=1, alpha=.01, eta0=.01, learning_rate='constant', loss='modified_huber', max_iter=1500, penalty='l1', tol=.001))\n",
    "add_clf(KNeighborsClassifier(n_neighbors=11, weights='uniform', p=1, leaf_size=15))\n",
    "# add_clf(LinearSVC(random_state=1, dual='auto', C=.1, max_iter=1000, penalty='l2', tol=1e-4))\n",
    "add_clf(RandomForestClassifier(random_state=1, max_depth=10, min_samples_leaf=2, n_estimators=100, min_samples_split=6), {}, None)\n",
    "# add_clf(GaussianNB())\n",
    "add_clf(AdaBoostClassifier(algorithm='SAMME.R', learning_rate=.1, random_state=1, n_estimators=400), {}, False)\n",
    "add_clf(NuSVC(random_state=1, kernel='poly', coef0=.5, nu=.4, probability=True))\n",
    "add_clf(ExtraTreesClassifier(random_state=1, max_depth=10, min_samples_leaf=1, min_samples_split=5, n_estimators=250), {}, None)\n",
    "add_clf(LinearDiscriminantAnalysis())\n",
    "add_clf(MLPClassifier(random_state=1, max_iter=1500))\n",
    "\n",
    "\n",
    "\n",
    "clf_labels = [model.__class__.__name__ for model in all_clf]\n",
    "\n",
    "tuned_clfs = []\n",
    "\n",
    "# Tune each classifier...\n",
    "for i, clf in enumerate(all_clf):\n",
    "    scaler = scalers[i]\n",
    "    name = clf.__class__.__name__ \n",
    "    hyper_params = hyper_param_grid[name]\n",
    "\n",
    "    if scaler:\n",
    "        clf = Pipeline([('sc', scaler()), (name, clf)])\n",
    "\n",
    "    grid_search = GridSearchCV(clf, param_grid=hyper_params, cv=5, n_jobs=-1)\n",
    "    grid_search.fit(X_train, y_train)\n",
    "    print(f\"Best parameters for {name}: {grid_search.best_params_} score={grid_search.best_score_}\")\n",
    "    tuned_clfs.append(grid_search.best_estimator_)\n",
    "\n",
    "mv_hard_clf = VotingClassifier(estimators=list(zip(clf_labels, tuned_clfs)), voting='hard')\n",
    "mv_hard_clf.fit(X_train, y_train)\n",
    "accuracy = mv_hard_clf.score(X_train, y_train)\n",
    "print(f\"Accuracy of the hard voting classifier: {accuracy}\")\n",
    "\n",
    "predictions = mv_hard_clf.predict(X_test)\n",
    "output = pd.DataFrame({'PassengerId': test_data.PassengerId, 'Survived': predictions})\n",
    "output.to_csv('submission_ensemble_hard.csv', index=False)\n",
    "\n",
    "mv_soft_clf = VotingClassifier(estimators=list(zip(clf_labels, tuned_clfs)), voting='soft')\n",
    "mv_soft_clf.fit(X_train, y_train)\n",
    "accuracy = mv_soft_clf.score(X_train, y_train)\n",
    "print(f\"Accuracy of the soft voting classifier: {accuracy}\")\n",
    "\n",
    "predictions = mv_soft_clf.predict(X_test)\n",
    "output = pd.DataFrame({'PassengerId': test_data.PassengerId, 'Survived': predictions})\n",
    "output.to_csv('submission_ensemble_soft.csv', index=False)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
